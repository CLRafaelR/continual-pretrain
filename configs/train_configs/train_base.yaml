model:
  model: microsoft/Phi-3-mini-128k-instruct
  tokenizer: microsoft/Phi-3-mini-128k-instruct
  use_cache: False
  max_length: 512


train: # huggingfaceのTrainingArgumentsで利用
  output_dir: ./outputs
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: steps
  learning_rate: 1e-6
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 256 # per_device_train_bath_size*gradient_accumulation_steps=256
  gradient_checkpointing: True
  weight_decay: 0.01 # 適当
  warmup_ratio: 0.1 # 適当
  optim: adamw_bnb_8bit # adamw_torch
  fp16: False
  bf16: True
  dataloader_num_workers: 1
  eval_steps: 50
  save_steps: 100
  logging_steps: 5
  run_name: test # wandbのプロジェクト名
  save_total_limit: 2
  save_on_each_node: False
  neftune_noise_alpha: 5 # NEFTTune　適当
  # deepspeed: ./configs/deepspeed/ds_config_zero2.json
  report_to: wandb
  # https://zenn.dev/selllous/articles/transformers_pretrain_to_ft#%E5%AD%A6%E7%BF%92%E9%80%9F%E5%BA%A6%E9%AB%98%E9%80%9F%E5%8C%96
  torch_compile: True
  logging_dir: ./outputs/log
  
seed: 42

dataset:
  path: hotchpotch/wikipedia-ja-20231030
  subset: chunked #!!null
  split: train
  cache_dir: /mnt/d/huggingface/datasets
